services:
  lomas_server:
    image: dsccadminch/lomas_server_dev
    build:
      dockerfile: ./Dockerfile
      target: lomas_server_dev
      context: ../
    container_name: lomas_server_dev
    ports:
      - 80:80
    restart: always
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT="http://opentelemetry-collector:4317"
    volumes:
      - ./lomas_server:/code/lomas_server
      - ../core/lomas_core:/code/lomas_core
      - ./configs/example_config.yaml:/usr/lomas_server/runtime.yaml
      - ./configs/example_secrets_local.yaml:/usr/lomas_server/secrets.yaml
      - ./data/collections/metadata:/data/collections/metadata/
      - ./data/collections/dataset_collection_local.yaml:/data/collections/dataset_collection.yaml
      - ./data/collections/user_collection.yaml:/data/collections/user_collection.yaml
    depends_on:
      minio-client:
        condition: service_completed_successfully
        restart: true
      mongodb:
        condition: service_started
        restart: true
      opentelemetry-collector:
        condition: service_started
    networks:
      - lomas-network

  mongodb:
    container_name: mongodb
    image: mongodb/mongodb-community-server:6.0-ubi8
    # Use only if volume is not a docker volume but a bind mount (e.g. ./data/:/data/db/)
    # The reason has to do with permissions (see https://stackoverflow.com/questions/29245216/write-in-shared-volumes-docker/29251160#29251160)
    # Still unclear why just setting chmod 777 ./data does not solve the issue.
    #user: 1000:1000
    ports:
      - 27017:27017
    networks:
      - lomas-network
    volumes:
      - mongodata:/data/db/
      - ./configs/mongodb_init.js:/docker-entrypoint-initdb.d/mongodb_init.js:ro
    # We use this in docker compose to have a user/password for mongodb
    # Proper deployment on a kubernetes cluster should use a helm chart
    # and parametrize a user specific to the fastAPI server.
    environment:
      - MONGODB_INITDB_ROOT_USERNAME=root
      - MONGODB_INITDB_ROOT_PASSWORD=root_pwd
      - MONGODB_INITDB_DATABASE=defaultdb
      # add user https://stackoverflow.com/questions/42912755/how-to-create-a-db-for-mongodb-container-on-start-up/54064268#54064268

  minio:
    container_name: minio
    image: minio/minio
    ports:
      - 9000:9000
      - 9001:9001
    command: server /data
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=admin123
      - MINIO_CONSOLE_ADDRESS=:9001
    volumes:
      - minio-data:/data
    networks:
      - lomas-network

  minio-client:
    container_name: minio-client
    image: minio/mc
    depends_on:
      - minio
    entrypoint: /bin/sh -c "
      sleep 15 && 
      mc alias set myminio http://minio:9000 admin admin123 && 
      mc mb myminio/loki-data || true &&
      mc mb myminio/loki-ruler || true &&
      mc mb myminio/example || true && 
      mc cp /data/datasets/titanic.csv myminio/example/data/titanic.csv &&
      mc cp /data/collections/metadata/titanic_metadata.yaml myminio/example/metadata/titanic_metadata.yaml &&
      mc ls --recursive --versions myminio/example"
    volumes:
      - ./data/:/data
    networks:
      - lomas-network

  lomas_admin_dashboard:
    image: dsccadminch/lomas_admin_dashboard_dev
    build:
      dockerfile: ./Dockerfile
      target: lomas_admin_dashboard_dev
      context: ../
    container_name: lomas_admin_dashboard_dev
    ports:
      - 8501:8501
    restart: always
    volumes:
      - ../core/lomas_core:/code/lomas_core
      - ./lomas_server:/code/lomas_server
      - ./configs/example_config.yaml:/usr/lomas_server/runtime.yaml
      - ./configs/example_secrets.yaml:/usr/lomas_server/secrets.yaml
      - ./configs/example_dashboard_config.yaml:/usr/lomas_dashboard/dashboard.yaml
      - ./configs/example_dashboard_server_config.toml:/code/.streamlit/config.toml
      - ./data/:/data/
    depends_on:
      - lomas_server
    networks:
      - lomas-network

  client:
    image: dsccadminch/lomas_client_dev
    build:
      dockerfile: ./Dockerfile
      target: lomas_client_dev
      context: ../
    container_name: lomas_client
    ports:
      - 8888:8888
    volumes:
      - ../client/lomas_client/:/code/lomas_client
      - ../core/lomas_core/:/code/lomas_core
      - ../client/configs/:/root/.jupyter/
      - ../client/notebooks/:/code/notebooks/
      - ./data/:/data/
    depends_on:
      - lomas_server
    networks:
      - lomas-network
  
  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./configs/observability/example_prometheus.yaml:/etc/prometheus/prometheus.yaml
      - prometheus-data:/prometheus
    restart: always
    networks:
      - lomas-network

  # Loki for log collection
  loki:
    image: grafana/loki:latest
    container_name: loki
    volumes:
      - ./configs/observability/example_loki.yaml:/etc/loki/config.yaml
    command: "-config.file=/etc/loki/config.yaml"
    ports:
      - "3100:3100"
      - "7946:7946"
      - "9095:9095"
    networks:
      - lomas-network
    depends_on:
      - minio-client

  # # Jaeger for trace collection
  # jaeger:
  #   image: jaegertracing/all-in-one:latest
  #   container_name: jaeger
  #   ports:
  #     - "16686:16686"
  #     - "14268"
  #     - "14250"
  #   restart: always
  #   networks:
  #     - lomas-network

  # OpenTelemetry Collector for receiving traces, metrics, and logs
  opentelemetry-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: opentelemetry-collector
    ports:
      - "4317:4317"  # OTLP gRPC port
      - "4381:4318"  # OTLP HTTP port
      - "9464:9464"  # Prometheus scraping port
    environment:
      - OTEL_SERVICE_NAME="lomas_server"
      - OTEL_SERVICE_VERSION="0.3.5"
      - OTEL_EXPORTER_OTLP_ENDPOINT="http://opentelemetry-collector:4317"
      - OTEL_METRICS_EXPORTER="prometheus"
      - OTEL_EXPORTER_PROMETHEUS_PORT="9464"
      - OTEL_METRICS_EXPORTER_LABELS="env=dev,app=lomas_server"
      - OTEL_TRACES_EXPORTER=jaeger
      - OTEL_EXPORTER_JAEGER_ENDPOINT=http://jaeger:14250
      - OTEL_LOGS_EXPORTER="loki"
      - OTEL_EXPORTER_LOKI_ENDPOINT=http://loki:3100/loki/api/v1/push
      - OTEL_LOG_LEVEL="debug"
      - OTEL_ENVIRONMENT="development"
      - OTEL_PYTHON_LOG_CORRELATION=true
    volumes:
      - ./configs/observability/example_otel_collector_config.yaml:/etc/otel_collector_config.yaml
    command:
      ["--config", "/etc/otel_collector_config.yaml"]
    restart: always
    networks:
      - lomas-network
    depends_on:
      - prometheus
      - loki
      # - jaeger

  # Grafana for dashboarding
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - ./configs/observability/example_grafana_dashboard.yaml:/etc/grafana/provisioning/datasources/ds.yaml
    environment:
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor
    networks:
      - lomas-network
    depends_on:
      - prometheus
      - loki
      # - jaeger

networks:
  lomas-network:
    driver: bridge

volumes:
  # These volumes needs to be setup only once with "docker volume create <volume_name>"
  # They will be stored in /var/lib/docker/volumes and managed by docker.
  mongodata:
    external: true
  minio-data:
    external: true
  prometheus-data:
    external: false