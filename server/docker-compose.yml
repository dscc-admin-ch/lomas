services:
  lomas_server:
    image: dsccadminch/lomas_server_dev
    build:
      dockerfile: ./Dockerfile
      target: lomas_server_dev
      context: ../
    container_name: lomas_server_dev
    ports:
      - 80:80
    restart: always
    environment:
      - SERVER_SERVICE_NAME=lomas-server-app
      - OTEL_COLLECTOR_ENDPOINT=http://otel-collector:4317
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_PYTHON_LOG_CORRELATION=true
      - OTEL_PYTHON_LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
    volumes:
      - ./lomas_server:/code/lomas_server
      - ../core/lomas_core:/code/lomas_core
      - ./configs/example_config.yaml:/usr/lomas_server/runtime.yaml
      - ./configs/example_secrets_local.yaml:/usr/lomas_server/secrets.yaml
      - ./data/:/data/
    depends_on:
      minio-client:
        condition: service_completed_successfully
        restart: true
      mongodb:
        condition: service_started
        restart: true
<<<<<<< HEAD
      otel-collector:
        condition: service_started
=======
      keycloak:
        condition: service_started
        restart: true
      keycloak_setup:
        condition: service_completed_successfully
        restart: true
>>>>>>> c96346d4 (Added jwt authentication method to server + option to choose authentication method. Updated client to work with both methods)
    networks:
      - lomas-network

  mongodb:
    container_name: mongodb
    image: mongodb/mongodb-community-server:6.0-ubi8
    # Use only if volume is not a docker volume but a bind mount (e.g. ./data/:/data/db/)
    # The reason has to do with permissions (see https://stackoverflow.com/questions/29245216/write-in-shared-volumes-docker/29251160#29251160)
    # Still unclear why just setting chmod 777 ./data does not solve the issue.
    #user: 1000:1000
    ports:
      - 27017:27017
    networks:
      - lomas-network
    volumes:
      - mongodata:/data/db/
      - ./configs/mongodb_init.js:/docker-entrypoint-initdb.d/mongodb_init.js:ro
    # We use this in docker compose to have a user/password for mongodb
    # Proper deployment on a kubernetes cluster should use a helm chart
    # and parametrize a user specific to the fastAPI server.
    environment:
      - MONGODB_INITDB_ROOT_USERNAME=root
      - MONGODB_INITDB_ROOT_PASSWORD=root_pwd
      - MONGODB_INITDB_DATABASE=defaultdb
      # add user https://stackoverflow.com/questions/42912755/how-to-create-a-db-for-mongodb-container-on-start-up/54064268#54064268

  mongodb-exporter:
    image: bitnami/mongodb-exporter:0.43.1
    container_name: mongodb-exporter
    ports:
      - 9216:9216
    networks:
      - lomas-network
    environment:
      - MONGODB_URI=mongodb://root:root_pwd@mongodb:27017
    depends_on:
      - mongodb
  
  keycloak:
    container_name: keycloak
    image: quay.io/keycloak/keycloak:26.1.0
    environment:
      - KC_BOOTSTRAP_ADMIN_USERNAME=admin
      - KC_BOOTSTRAP_ADMIN_PASSWORD=admin
    command: start-dev --health-enabled=true # TODO will have to change that to production mode.
    ports:
        - 8081:8080
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/9000;echo -e 'GET /health/ready HTTP/1.1\r\nhost: http://localhost\r\nConnection: close\r\n\r\n' >&3;if [ $? -eq 0 ]; then echo 'Healthcheck Successful';exit 0;else echo 'Healthcheck Failed';exit 1;fi;"]
      interval: 10s
      retries: 10
      timeout: 5s
    restart: always
    networks:
      - lomas-network

  keycloak_setup:
    container_name: keycloak_setup
    build:
      context: ./utility_scripts/keycloak_setup/
    depends_on:
      keycloak:
        condition: service_healthy
    entrypoint: /bin/sh -c "python keycloak_setup.py"
    environment:
      OAUTHLIB_INSECURE_TRANSPORT: 1 # ! Not for production !
    env_file: ./utility_scripts/keycloak_setup/.env
    volumes:
      - ./utility_scripts/keycloak_setup/keycloak_setup.py:/app/keycloak_setup.py
    networks:
      - lomas-network

  minio:
    container_name: minio
    image: minio/minio
    ports:
      - 19000:19000
      - 19001:19001
    command: server /data
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=admin123
      - MINIO_CONSOLE_ADDRESS=:19001
      - MINIO_ADDRESS=:19000
    volumes:
      - minio-data:/data
    networks:
      - lomas-network

  minio-client:
    container_name: minio-client
    image: minio/mc
    depends_on:
      - minio
    entrypoint: /bin/sh -c "
      sleep 15 &&
      mc alias set myminio http://minio:19000 admin admin123 &&
      mc mb myminio/example || true &&
      mc cp /data/datasets/titanic.csv myminio/example/data/titanic.csv &&
      mc cp /data/collections/metadata/titanic_metadata.yaml myminio/example/metadata/titanic_metadata.yaml &&
      mc ls --recursive --versions myminio/example"
    volumes:
      - ./data/:/data
    networks:
      - lomas-network

  lomas_admin_dashboard:
    image: dsccadminch/lomas_admin_dashboard_dev
    build:
      dockerfile: ./Dockerfile
      target: lomas_admin_dashboard_dev
      context: ../
    container_name: lomas_admin_dashboard_dev
    ports:
      - 8501:8501
    restart: always
    volumes:
      - ../core/lomas_core:/code/lomas_core
      - ./lomas_server:/code/lomas_server
      - ./configs/example_config.yaml:/usr/lomas_server/runtime.yaml
      - ./configs/example_secrets.yaml:/usr/lomas_server/secrets.yaml
      - ./configs/example_dashboard_config.yaml:/usr/lomas_dashboard/dashboard.yaml
      - ./configs/example_dashboard_server_config.toml:/code/.streamlit/config.toml
      - ./data/:/data/
    depends_on:
      - lomas_server
    networks:
      - lomas-network

  client:
    image: dsccadminch/lomas_client_dev
    build:
      dockerfile: ./Dockerfile
      target: lomas_client_dev
      context: ../
    container_name: lomas_client
    environment:
      - CLIENT_SERVICE_NAME=lomas-client
      - OTEL_COLLECTOR_ENDPOINT=http://otel-collector:4317
    ports:
      - 8888:8888
    volumes:
      - ../client/lomas_client/:/code/lomas_client
      - ../core/lomas_core/:/code/lomas_core
      - ../client/configs/:/root/.jupyter/
      - ../client/notebooks/:/code/notebooks/
      - ./data/:/data/
    depends_on:
      - lomas_server
    networks:
      - lomas-network

  tempo:
    image: grafana/tempo:main-265ca32
    container_name: tempo
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./configs/observability/example_tempo_config.yaml:/etc/tempo.yaml
    expose:
      - 3200
      - 4317
    networks:
      - lomas-network

  otel-collector:
    image: otel/opentelemetry-collector:0.119.0
    container_name: otel-collector
    ports:
      - "4317:4317"
      - "4318:4318"
      - "13133:13133" # health_check extension
      - "1777:1777" # pprof extension
      - "55679:55679" # zpages extension
      - "9091:9090" # prometheus
    volumes:
      - ./configs/observability/example_otel_config.yaml:/etc/otel/config.yaml
    command: ["--config=/etc/otel/config.yaml"]
    depends_on:
      tempo:
        condition: service_started
      loki:
        condition: service_started
    networks:
      - lomas-network

  loki:
    image: grafana/loki:3.4
    container_name: loki
    ports:
      - "3100:3100"
    command:
      - "-config.file=/etc/loki/local-config.yaml"
    volumes:
      - ./configs/observability/example_loki_config.yaml:/etc/loki/local-config.yaml
    networks:
      - lomas-network

  prometheus:
    image: prom/prometheus:v3.2.0
    container_name: prometheus
    command:
      - --config.file=/etc/prometheus.yaml
    volumes:
      - ./configs/observability/example_prometheus_config.yaml:/etc/prometheus.yaml
    ports:
      - "9090:9090"
    networks:
      - lomas-network

  grafana:
    image: grafana/grafana:11.0.0
    container_name: grafana
    volumes:
      - ./configs/observability/grafana/example_datasources_config.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
    ports:
      - "3000:3000"
    networks:
      - lomas-network

networks:
  lomas-network:
    driver: bridge
    name: lomas-network

volumes:
  # These volumes needs to be setup only once with "docker volume create <volume_name>"
  # They will be stored in /var/lib/docker/volumes and managed by docker.
  mongodata:
    external: true
  minio-data:
    external: true
