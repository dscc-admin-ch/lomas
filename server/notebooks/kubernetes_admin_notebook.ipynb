{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363c238d-5925-4b4b-8f68-8ad84ea4705b",
   "metadata": {},
   "source": [
    "# Secure Data Disclosure on Kubernetes: Deployment and Server Administration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1363b-e87e-4d0e-bb3f-9af1a1b72b8d",
   "metadata": {},
   "source": [
    "This notebook showcases how a data owner could set up the service on a kubernetes cluster, add and make their data available to certain user. We will do this in a step by step fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de384c88-559e-4384-a49b-1664ffdd6692",
   "metadata": {},
   "source": [
    "## Deploying the service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba5946",
   "metadata": {},
   "source": [
    "### Building the server image\n",
    "The Lomas service is comprised of a fastapi server and a MongoDB database for keeping state and administration. While the database image is public, the server image must first be built and pushed to a registry.\n",
    "\n",
    "NOTE: For now, the server configuration file is copied and put into the server container. This is of course not practical (and not safe, since the configuration file contains passwords and secrets) and will be updated in future versions. The `config/example_config.yaml` is the one that is copied into the container. One has to change it and rebuild+push the server container in order to change the server configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f688134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker login (=> use personal token from dockerhub, has to be done only once)\n",
    "\n",
    "!cd .. && docker build --target lomas_server -t <your_registry>/lomas_serverer:latest .\n",
    "!cd .. && docker push <your_registry>/lomas-server:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3237b-6f13-4c52-a9f2-82d94f0b7e66",
   "metadata": {},
   "source": [
    "### Deploying the service Helm chart\n",
    "We use a Helm chart to deploy the service on a Kubernetes cluster. The lomas-server chart is located at `deploy/helm/charts/lomas_server`, let us change our working directory to this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e249d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../deploy/helm/charts/lomas_server')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11075ea0",
   "metadata": {},
   "source": [
    "The `values.yaml` file contains all the configuration values for the service. We must now update the `image.repository` field to the one we pushed the server container image to. One can also change the url to which the service will be published with `ingress.hosts[0].host` (or disable this feature by setting `ingress.enabled` to `False`).\n",
    "\n",
    "    => Update `values.yaml` file\n",
    "\n",
    "As previously stated, the service is made up of a server and a MongoDB database. Before installing the chart, we must thus first download that dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe550e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 1 charts\n",
      "Downloading mongodb from repo oci://registry-1.docker.io/bitnamicharts\n",
      "Save error occurred:  could not download oci://registry-1.docker.io/bitnamicharts/mongodb: failed to copy: httpReadSeeker: failed open: failed to do request: Get \"https://production.cloudflare.docker.com/registry-v2/docker/registry/v2/blobs/sha256/95/953b9cef6799e942255a1d5edcb7cb7508230fb57e4d68d02e27aed4b1694eaf/data?verify=1701701776-w82MF1fkjhlDOYT4WuSJHicDe5c%3D\": dial tcp: lookup production.cloudflare.docker.com on 169.254.25.10:53: server misbehaving\n",
      "Error: could not download oci://registry-1.docker.io/bitnamicharts/mongodb: failed to copy: httpReadSeeker: failed open: failed to do request: Get \"https://production.cloudflare.docker.com/registry-v2/docker/registry/v2/blobs/sha256/95/953b9cef6799e942255a1d5edcb7cb7508230fb57e4d68d02e27aed4b1694eaf/data?verify=1701701776-w82MF1fkjhlDOYT4WuSJHicDe5c%3D\": dial tcp: lookup production.cloudflare.docker.com on 169.254.25.10:53: server misbehaving\n"
     ]
    }
   ],
   "source": [
    "!helm dependency update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913bce4",
   "metadata": {},
   "source": [
    "Now the chart is ready to be installed, so let the magic happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed0e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n"
     ]
    }
   ],
   "source": [
    "!helm install -f values.yaml lomas-service ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d4837",
   "metadata": {},
   "source": [
    "The installation notes show the url at which the server is exposed. One can have a look at the api docummentation by visiting `<server_url>/docs`\n",
    "\n",
    "One can also check the whether the service started error free by using the `kubectl get all` command as well as inspecting the server logs with `kubectl logs <server-pod-name>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbebd54-8deb-46e6-b811-73ac74028569",
   "metadata": {},
   "source": [
    "## Administering the service by accessing the mongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ec36f",
   "metadata": {},
   "source": [
    "Let's switch directory again and move to the administration script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ede728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../../../src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c8115",
   "metadata": {},
   "source": [
    "To interact with the mongoDB, we will need to install a few libraries. Let's do so by creating a python virtual environment and installing the dependencies listed in `admin_requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6863a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annotated-types==0.5.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: anyio==3.7.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 2)) (3.7.1)\n",
      "Requirement already satisfied: asttokens==2.4.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 3)) (2.4.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 4)) (0.2.0)\n",
      "Collecting boto3 (from -r ../admin_requirements.txt (line 5))\n",
      "  Downloading boto3-1.34.40-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: comm==0.1.4 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 6)) (0.1.4)\n",
      "Requirement already satisfied: debugpy==1.7.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 7)) (1.7.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: dnspython==2.4.2 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 9)) (2.4.2)\n",
      "Requirement already satisfied: executing==1.2.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: fastapi==0.103.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 11)) (0.103.1)\n",
      "Requirement already satisfied: idna==3.4 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 12)) (3.4)\n",
      "Requirement already satisfied: ipykernel==6.25.2 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 13)) (6.25.2)\n",
      "Requirement already satisfied: ipython==8.15.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 14)) (8.15.0)\n",
      "Requirement already satisfied: jedi==0.19.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 15)) (0.19.0)\n",
      "Requirement already satisfied: jupyter_client==8.3.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 16)) (8.3.1)\n",
      "Requirement already satisfied: jupyter_core==5.3.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 17)) (5.3.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 18)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio==1.5.7 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 19)) (1.5.7)\n",
      "Requirement already satisfied: packaging==23.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 20)) (23.1)\n",
      "Requirement already satisfied: parso==0.8.3 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 21)) (0.8.3)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 22)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 23)) (0.7.5)\n",
      "Requirement already satisfied: platformdirs==3.10.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 24)) (3.10.0)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.39 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 25)) (3.0.39)\n",
      "Requirement already satisfied: psutil==5.9.5 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 26)) (5.9.5)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 27)) (0.7.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 28)) (0.2.2)\n",
      "Requirement already satisfied: pyaml==23.9.5 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 29)) (23.9.5)\n",
      "Requirement already satisfied: pydantic==2.3.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 30)) (2.3.0)\n",
      "Requirement already satisfied: pydantic_core==2.6.3 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 31)) (2.6.3)\n",
      "Requirement already satisfied: Pygments==2.16.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 32)) (2.16.1)\n",
      "Requirement already satisfied: pymongo==4.5.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 33)) (4.5.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 34)) (2.8.2)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 35)) (6.0.1)\n",
      "Requirement already satisfied: pyzmq==25.1.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 36)) (25.1.1)\n",
      "Requirement already satisfied: six==1.16.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 37)) (1.16.0)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 38)) (1.3.0)\n",
      "Requirement already satisfied: stack-data==0.6.2 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 39)) (0.6.2)\n",
      "Requirement already satisfied: starlette==0.27.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 40)) (0.27.0)\n",
      "Requirement already satisfied: tornado==6.3.3 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 41)) (6.3.3)\n",
      "Requirement already satisfied: traitlets==5.9.0 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 42)) (5.9.0)\n",
      "Requirement already satisfied: typing_extensions==4.7.1 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 43)) (4.7.1)\n",
      "Requirement already satisfied: wcwidth==0.2.6 in /home/onyxia/work/.venv/lib/python3.10/site-packages (from -r ../admin_requirements.txt (line 44)) (0.2.6)\n",
      "Requirement already satisfied: exceptiongroup in /home/onyxia/work/.venv/lib/python3.10/site-packages (from anyio==3.7.1->-r ../admin_requirements.txt (line 2)) (1.2.0)\n",
      "Collecting botocore<1.35.0,>=1.34.40 (from boto3->-r ../admin_requirements.txt (line 5))\n",
      "  Downloading botocore-1.34.40-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->-r ../admin_requirements.txt (line 5))\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->-r ../admin_requirements.txt (line 5))\n",
      "  Downloading s3transfer-0.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting urllib3<2.1,>=1.25.4 (from botocore<1.35.0,>=1.34.40->boto3->-r ../admin_requirements.txt (line 5))\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Downloading boto3-1.34.40-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.34.40-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.34.40 botocore-1.34.40 jmespath-1.0.1 s3transfer-0.10.0 urllib3-2.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../admin_requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35fd20-715c-483b-88e4-449c287ba61d",
   "metadata": {},
   "source": [
    "We should now have the required environment to interact with the admin database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368d6a6-f1fe-4f65-9ce1-38c0b39584d1",
   "metadata": {},
   "source": [
    "### Preparing the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c19b8-303d-4fe8-b515-33ed1099c581",
   "metadata": {},
   "source": [
    "You can visualise all the options offered by the database by running the command `python mongodb_admin.py --help`. We will go through through each of them in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a749f4b-93cb-460c-bb40-4880df6e51d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: MongoDB administration script for the user database [-h]\n",
      "                                                           {add_user,add_user_with_budget,del_user,add_dataset_to_user,del_dataset_to_user,set_budget_field,set_may_query,show_user,create_users_collection,add_dataset,add_datasets,drop_collection,show_collection}\n",
      "                                                           ...\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "subcommands:\n",
      "  {add_user,add_user_with_budget,del_user,add_dataset_to_user,del_dataset_to_user,set_budget_field,set_may_query,show_user,create_users_collection,add_dataset,add_datasets,drop_collection,show_collection}\n",
      "                        user database administration operations\n",
      "    add_user            add user to users collection\n",
      "    add_user_with_budget\n",
      "                        add user with budget to users collection\n",
      "    del_user            delete user from users collection\n",
      "    add_dataset_to_user\n",
      "                        add dataset with initialized budget values for a user\n",
      "    del_dataset_to_user\n",
      "                        delete dataset for user in users collection\n",
      "    set_budget_field    set budget field to given value for given user and\n",
      "                        dataset\n",
      "    set_may_query       set may query field to given value for given user\n",
      "    show_user           show all metadata of user\n",
      "    create_users_collection\n",
      "                        create users collection from yaml file\n",
      "    add_dataset         set in which database the dataset is stored\n",
      "    add_datasets        create dataset to database type collection\n",
      "    drop_collection     delete collection from database\n",
      "    show_collection     print the users collection\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579cbc3",
   "metadata": {},
   "source": [
    "Let's first make sure the database is empty and in a clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18a3681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted collection datasets.\n",
      "Deleted collection metadata.\n",
      "Deleted collection users.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py drop_collection --collection datasets\n",
    "!python mongodb_admin.py drop_collection --collection metadata\n",
    "!python mongodb_admin.py drop_collection --collection users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edd7d3-20f9-4546-afc8-25661f948d44",
   "metadata": {},
   "source": [
    "### Datasets (add and drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1597b3-767f-470c-a7d7-8fe41dd82da5",
   "metadata": {},
   "source": [
    "We first need to set the dataset meta-information. For each dataset, 2 informations are required:\n",
    "- the type of database in which the dataset is stored\n",
    "- a path to the metadata of the dataset (stored as a yaml file).\n",
    "\n",
    "To later perform query on the dataset, metadata are required. In this secure server the metadata information is expected to be in the same format as [SmartnoiseSQL dictionary format](https://docs.smartnoise.org/sql/metadata.html#dictionary-format), where among other, there is information about all the available columns, their type, bound values (see Smartnoise page for more details). It is also expected to be in a `yaml` file.\n",
    "\n",
    "These information (dataset name, dataset type and metadata path) are stored in the `datasets` collection. Then for each dataset, its metadata is fetched from its `yaml` file and stored in a collection named `metadata`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678fb3f",
   "metadata": {},
   "source": [
    "We then check that there is indeed no data in the dataset and metadata collections yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b7a7fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36e03ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d331ea",
   "metadata": {},
   "source": [
    "We can add **one dataset** with its name, and informations on where to load the dataset and metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53f5787d-e721-43d9-85ce-da842f173381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dataset PENGUIN with database REMOTE_HTTP_DB and associated metadata.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_dataset -d \"PENGUIN\" -db \"REMOTE_HTTP_DB\" -d_url \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\" -m_db \"LOCAL_DB\" -mp \"../data/collections/metadata/penguin_metadata.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c7b53",
   "metadata": {},
   "source": [
    "We can also show an example of how to add a dataset and its metadata stored on an S3 server. The access key id and secret access id should be updated depending on the location of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5604d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dataset TITANIC with database S3_DB and associated metadata.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_dataset -d \"TITANIC\" -db \"S3_DB\" -s3b \"example\" -s3k \"data/titanic.csv\" -s3_url \"https://api-lomas-minio.lab.sspcloud.fr\" -s3_ak \"admin\" -s3_sak \"admin123\" -m_db \"S3_DB\" -m_s3b \"example\" -m_s3k \"metadata/titanic_metadata.yaml\" -m_s3_url \"https://api-lomas-minio.lab.sspcloud.fr\" -m_s3_ak \"admin\" -m_s3_sak \"admin123\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f8990",
   "metadata": {},
   "source": [
    "We can now see the dataset and metadata collection with the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3005eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dataset_name': 'TITANIC', 'database_type': 'S3_DB', 's3_bucket': 'example', 's3_key': 'data/titanic.csv', 'endpoint_url': 'https://api-sdd-minio.lab.sspcloud.fr', 'aws_access_key_id': 'admin', 'aws_secret_access_key': 'admin123'}]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7527f3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'TITANIC': {'': {'Schema': {'Table': {'max_ids': 1, 'PassengerId': {'type': 'int', 'lower': 1}, 'Pclass': {'type': 'int', 'lower': 1, 'upper': 3}, 'Name': {'type': 'string'}, 'Sex': {'type': 'string', 'cardinality': 2, 'categories': ['male', 'female']}, 'Age': {'type': 'float', 'lower': 0.1, 'upper': 100.0}, 'SibSp': {'type': 'int', 'lower': 0}, 'Parch': {'type': 'int', 'lower': 0}, 'Ticket': {'type': 'string'}, 'Fare': {'type': 'float', 'lower': 0.0}, 'Cabin': {'type': 'string'}, 'Embarked': {'type': 'string', 'cardinality': 3, 'categories': ['C', 'Q', 'S']}, 'Survived': {'type': 'boolean'}, 'row_privacy': True}}}, 'engine': 'csv'}}]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2076e",
   "metadata": {},
   "source": [
    "Or a path to a yaml file which contains all these informations to do **multiple datasets** in one command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e42f9cb-3a02-45f5-baee-2e06edda739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning done. \n",
      "\n",
      "Added datasets collection from yaml at ../data/collections/dataset_collection.yaml. \n",
      "Added metadata of IRIS dataset. \n",
      "Added metadata of PENGUIN dataset. \n",
      "Added metadata of TITANIC dataset. \n",
      "Added metadata of FSO_INCOME_SYNTHETIC dataset. \n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_datasets --path ../data/collections/dataset_collection.yaml -c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a69223",
   "metadata": {},
   "source": [
    "The argument *-c* or *--clean* allows you to clear the current dataset collection before adding your collection.\n",
    "\n",
    "By default, *add_datasets* will only add new dataset found from the collection provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7047e416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata already exist. Use the command -om to overwrite with new values.\n",
      "Metadata already exist. Use the command -om to overwrite with new values.\n",
      "Metadata already exist. Use the command -om to overwrite with new values.\n",
      "Metadata already exist. Use the command -om to overwrite with new values.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_datasets --path ../data/collections/dataset_collection.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b06b96",
   "metadata": {},
   "source": [
    "Arguments :\n",
    "\n",
    "*-od* / *--overwrite_datasets* : Overwrite the values for **exisiting datasets** with the values provided in the yaml.\n",
    "\n",
    "*-om* / *--overwrite_metadata* : Overwrite the values for **exisiting metadata** with the values provided in the yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "692adbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets updated with values from yaml at ../data/collections/dataset_collection.yaml.\n",
      "Metadata already exist. Use the command -om to overwrite with new values.\n",
      "Metadata already exist. Use the command -om to overwrite with new values.\n",
      "Metadata already exist. Use the command -om to overwrite with new values.\n",
      "Metadata already exist. Use the command -om to overwrite with new values.\n"
     ]
    }
   ],
   "source": [
    "# Add new datasets/metadata, update existing datasets\n",
    "!python mongodb_admin.py add_datasets --path ../data/collections/dataset_collection.yaml -od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2547d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata updated for dataset : IRIS.\n",
      "Metadata updated for dataset : PENGUIN.\n",
      "Metadata updated for dataset : TITANIC.\n",
      "Metadata updated for dataset : FSO_INCOME_SYNTHETIC.\n"
     ]
    }
   ],
   "source": [
    "# Add new datasets/metadata, update existing metadata\n",
    "!python mongodb_admin.py add_datasets --path ../data/collections/dataset_collection.yaml -om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bc8ea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets updated with values from yaml at ../data/collections/dataset_collection.yaml.\n",
      "Metadata updated for dataset : IRIS.\n",
      "Metadata updated for dataset : PENGUIN.\n",
      "Metadata updated for dataset : TITANIC.\n",
      "Metadata updated for dataset : FSO_INCOME_SYNTHETIC.\n"
     ]
    }
   ],
   "source": [
    "# Add new datasets/metadata, update existing datasets & metadata\n",
    "!python mongodb_admin.py add_datasets --path ../data/collections/dataset_collection.yaml -od -om"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d686ae",
   "metadata": {},
   "source": [
    "Let's see all the dataset collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "536b5b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dataset_name': 'IRIS', 'database_type': 'REMOTE_HTTP_DB', 'dataset_url': 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv', 'metadata': {'database_type': 'LOCAL_DB', 'metadata_path': '../data/collections/metadata/iris_metadata.yaml'}}, {'dataset_name': 'PENGUIN', 'database_type': 'REMOTE_HTTP_DB', 'dataset_url': 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv', 'metadata': {'database_type': 'LOCAL_DB', 'metadata_path': '../data/collections/metadata/penguin_metadata.yaml'}}, {'dataset_name': 'TITANIC', 'database_type': 'S3_DB', 's3_bucket': 'example', 's3_key': 'data/titanic.csv', 'endpoint_url': 'https://api-sdd-minio.lab.sspcloud.fr', 'aws_access_key_id': 'admin', 'aws_secret_access_key': 'admin123', 'metadata': {'database_type': 'LOCAL_DB', 'metadata_path': '../data/collections/metadata/titanic_metadata.yaml'}}, {'dataset_name': 'FSO_INCOME_SYNTHETIC', 'database_type': 'LOCAL_DB', 'dataset_path': '../data/datasets/income_synthetic_data.csv', 'metadata': {'database_type': 'LOCAL_DB', 'metadata_path': '../data/collections/metadata/fso_income_synthetic_metadata.yaml'}}]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0746b382-8692-445f-9ca9-0d2407a25259",
   "metadata": {},
   "source": [
    "Finally let's have a look at the  stored metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c667dda0-5d0f-48c8-956c-8d8a756b7ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'IRIS': {'': {'Schema': {'Table': {'max_ids': 1, 'petal_length': {'type': 'float', 'lower': 0.5, 'upper': 10.0}, 'petal_width': {'type': 'float', 'lower': 0.05, 'upper': 5.0}, 'row_privacy': True, 'sepal_length': {'type': 'float', 'lower': 2.0, 'upper': 10.0}, 'sepal_width': {'type': 'float', 'lower': 1.0, 'upper': 6.0}, 'species': {'type': 'string', 'cardinality': 3, 'categories': ['setosa', 'versicolor', 'virginica']}}}}, 'engine': 'csv'}}, {'PENGUIN': {'': {'Schema': {'Table': {'max_ids': 1, 'row_privacy': True, 'censor_dims': False, 'species': {'type': 'string', 'cardinality': 3, 'categories': ['Adelie', 'Chinstrap', 'Gentoo']}, 'island': {'type': 'string', 'cardinality': 3, 'categories': ['Torgersen', 'Biscoe', 'Dream']}, 'bill_length_mm': {'type': 'float', 'lower': 30.0, 'upper': 65.0}, 'bill_depth_mm': {'type': 'float', 'lower': 13.0, 'upper': 23.0}, 'flipper_length_mm': {'type': 'float', 'lower': 150.0, 'upper': 250.0}, 'body_mass_g': {'type': 'float', 'lower': 2000.0, 'upper': 7000.0}, 'sex': {'type': 'string', 'cardinality': 2, 'categories': ['MALE', 'FEMALE']}}}}, 'engine': 'csv'}}, {'TITANIC': {'': {'Schema': {'Table': {'max_ids': 1, 'PassengerId': {'type': 'int', 'lower': 1}, 'Pclass': {'type': 'int', 'lower': 1, 'upper': 3}, 'Name': {'type': 'string'}, 'Sex': {'type': 'string', 'cardinality': 2, 'categories': ['male', 'female']}, 'Age': {'type': 'float', 'lower': 0.1, 'upper': 100.0}, 'SibSp': {'type': 'int', 'lower': 0}, 'Parch': {'type': 'int', 'lower': 0}, 'Ticket': {'type': 'string'}, 'Fare': {'type': 'float', 'lower': 0.0}, 'Cabin': {'type': 'string'}, 'Embarked': {'type': 'string', 'cardinality': 3, 'categories': ['C', 'Q', 'S']}, 'Survived': {'type': 'boolean'}, 'row_privacy': True}}}, 'engine': 'csv'}}, {'FSO_INCOME_SYNTHETIC': {'': {'Schema': {'Table': {'max_ids': 1, 'region': {'type': 'int'}, 'eco_branch': {'type': 'int'}, 'profession': {'type': 'int'}, 'education': {'type': 'int'}, 'age': {'type': 'int'}, 'sex': {'type': 'int'}, 'income': {'type': 'float', 'lower': 1000, 'upper': 100000}}}}, 'engine': 'csv'}}]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b85d5",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab18db-4b6d-4663-bde0-b5d9d3d3d2ee",
   "metadata": {},
   "source": [
    "#### Adding users\n",
    "Let's see which users are alreay loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f450145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ae627",
   "metadata": {},
   "source": [
    "And now let's add few users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f6aa33c-6bd1-4d62-ba06-3533b064340d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added access to user Mrs. Daisy with dataset IRIS, budget epsilon 10.0 and delta 0.001.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_user_with_budget --user 'Mrs. Daisy' --dataset 'IRIS' --epsilon 10.0 --delta 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7858f019-8783-4fed-acd8-ff0107d33465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added access to user Mr. Coldheart with dataset PENGUIN, budget epsilon 10.0 and delta 0.001.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_user_with_budget --user 'Mr. Coldheart' --dataset 'PENGUIN' --epsilon 10.0 --delta 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "231e7d93-05ba-424a-8329-d96b0bfb4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added access to user Lord McFreeze with dataset PENGUIN, budget epsilon 10.0 and delta 0.001.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_user_with_budget --user 'Lord McFreeze' --dataset 'PENGUIN' --epsilon 10.0 --delta 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0c274-880c-44f9-9182-6cb162a54c55",
   "metadata": {},
   "source": [
    "Users must all have different names, otherwise you will have an error and nothing will be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6276730e-39c2-47f1-962f-342c1acb7944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/onyxia/work/sdd-poc-server/server/src/mongodb_admin.py\", line 549, in <module>\n",
      "    args.func(args)\n",
      "  File \"/home/onyxia/work/sdd-poc-server/server/src/mongodb_admin.py\", line 47, in add_user_with_budget\n",
      "    raise ValueError(\"Cannot add user because already exists. \")\n",
      "ValueError: Cannot add user because already exists. \n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_user_with_budget --user 'Lord McFreeze' --dataset 'IRIS' --epsilon 10.0 --delta 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f81f7e-e086-412f-8467-89b665e5559a",
   "metadata": {},
   "source": [
    "If you want to add another dataset access to an existing user, just use the function `add_dataset_to_user` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82a5f498-aed1-4779-9d73-b2b71dde4ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added access to dataset IRIS to user Lord McFreeze with budget epsilon 5.0 and delta 0.005.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_dataset_to_user --user 'Lord McFreeze' --dataset 'IRIS' --epsilon 5.0 --delta 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06170073-49ed-4329-8101-2debdd77eb98",
   "metadata": {},
   "source": [
    "Alternatively, you can create a user without assigned dataset and then add dataset in another command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06839270-36cf-4de7-b93c-d143c4866bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user Madame Frostina.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_user --user 'Madame Frostina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e83378fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added access to dataset IRIS to user Madame Frostina with budget epsilon 5.0 and delta 0.005.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_dataset_to_user --user 'Madame Frostina' --dataset 'IRIS' --epsilon 5.0 --delta 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "919b2652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added access to dataset PENGUIN to user Madame Frostina with budget epsilon 5.0 and delta 0.005.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_dataset_to_user --user 'Madame Frostina' --dataset 'PENGUIN' --epsilon 5.0 --delta 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed2714",
   "metadata": {},
   "source": [
    "And we can also modify existing the total budget of a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec2cce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added access to user Dr. Antartica with dataset PENGUIN, budget epsilon 10.0 and delta 0.001.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py add_user_with_budget --user 'Dr. Antartica' --dataset 'PENGUIN' --epsilon 10.0 --delta 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87eecb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set budget of Dr. Antartica for dataset PENGUIN of initial_epsilon to 20.0.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py set_budget_field --user 'Dr. Antartica' --dataset 'PENGUIN' --field initial_epsilon --value 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb5dc2-e91e-4440-8df5-3e9506bf4ee1",
   "metadata": {},
   "source": [
    "Let's see the current state of the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b3f61c6-65dc-4b1e-a32e-47cdd2729ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_name': 'Mrs. Daisy', 'may_query': True, 'datasets_list': [{'dataset_name': 'IRIS', 'initial_epsilon': 10.0, 'initial_delta': 0.001, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Mr. Coldheart', 'may_query': True, 'datasets_list': [{'dataset_name': 'PENGUIN', 'initial_epsilon': 10.0, 'initial_delta': 0.001, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Lord McFreeze', 'may_query': True, 'datasets_list': [{'dataset_name': 'PENGUIN', 'initial_epsilon': 10.0, 'initial_delta': 0.001, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}, {'dataset_name': 'IRIS', 'initial_epsilon': 5.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Madame Frostina', 'may_query': True, 'datasets_list': [{'dataset_name': 'IRIS', 'initial_epsilon': 5.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}, {'dataset_name': 'PENGUIN', 'initial_epsilon': 5.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Dr. Antartica', 'may_query': True, 'datasets_list': [{'dataset_name': 'PENGUIN', 'initial_epsilon': 20.0, 'initial_delta': 0.001, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ae62f-ff80-4234-8102-4dccec0b284f",
   "metadata": {},
   "source": [
    "Do not hesitate to re-run this command after every other command to ensure that everything runs as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1f5ba-68bd-4c96-bacd-b81dfa5d6302",
   "metadata": {},
   "source": [
    "#### Removing users\n",
    "You have just heard that the penguin named Coldheart might have malicious intentions and decide to remove his access until an investigation has been carried out. To ensure that he is not allowed to do any more queries, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f341b3d-5a88-4fd9-8c97-cc70145834f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set user Mr. Coldheart may query.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py set_may_query --user 'Mr. Coldheart' --value False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc56586-f9a9-4e88-abed-51ba36a6e4f1",
   "metadata": {},
   "source": [
    "Now, he won't be able to do any query (unless you re-run the query with --value True).\n",
    "\n",
    "A few days have passed and the investigation reveals that he was aiming to do unethical research, you can remove his dataset by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9153d9af-b4be-4496-9f80-d140870f60fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove access to dataset PENGUIN from user Mr. Coldheart.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py del_dataset_to_user --user 'Mr. Coldheart' --dataset 'PENGUIN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d411ae-a211-4997-8984-81281c6275eb",
   "metadata": {},
   "source": [
    "Or delete him completely from the codebase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a54e89eb-1ee1-48ad-9e00-bace8516a3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted user Mr. Coldheart.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py del_user --user 'Mr. Coldheart'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7c17f-da34-472a-ad7f-3ae73a1beb7b",
   "metadata": {},
   "source": [
    "Let's see the resulting users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79fa414a-f097-4207-a628-19fa434a1ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_name': 'Mrs. Daisy', 'may_query': True, 'datasets_list': [{'dataset_name': 'IRIS', 'initial_epsilon': 10.0, 'initial_delta': 0.001, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Lord McFreeze', 'may_query': True, 'datasets_list': [{'dataset_name': 'PENGUIN', 'initial_epsilon': 10.0, 'initial_delta': 0.001, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}, {'dataset_name': 'IRIS', 'initial_epsilon': 5.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Madame Frostina', 'may_query': True, 'datasets_list': [{'dataset_name': 'IRIS', 'initial_epsilon': 5.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}, {'dataset_name': 'PENGUIN', 'initial_epsilon': 5.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Dr. Antartica', 'may_query': True, 'datasets_list': [{'dataset_name': 'PENGUIN', 'initial_epsilon': 20.0, 'initial_delta': 0.001, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a46a59-70ed-4a26-88cd-6ca8f1d17318",
   "metadata": {},
   "source": [
    "#### Changing the budget\n",
    "You also change your mind about the budget allowed to Lord McFreeze and give him a bit more on the penguin dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0909e6c4-141e-4d57-acd2-bdc0a2d92cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set budget of Lord McFreeze for dataset PENGUIN of initial_epsilon to 15.0.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py set_budget_field --user 'Lord McFreeze' --dataset 'PENGUIN' --field initial_epsilon --value 15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0e110fe-4297-4559-9a95-bc0ebdfa402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set budget of Lord McFreeze for dataset PENGUIN of initial_delta to 0.005.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py set_budget_field --user 'Lord McFreeze' --dataset 'PENGUIN' --field initial_delta --value 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d7ed4-ce1d-4a87-9319-6b57968ef20e",
   "metadata": {},
   "source": [
    "Let's check all our changes by looking at the state of the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ab46c5d-1553-4925-bd25-61c9c205dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_name': 'Mrs. Daisy', 'may_query': True, 'datasets_list': [{'dataset_name': 'IRIS', 'initial_epsilon': 10.0, 'initial_delta': 0.001, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Lord McFreeze', 'may_query': True, 'datasets_list': [{'dataset_name': 'PENGUIN', 'initial_epsilon': 15.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}, {'dataset_name': 'IRIS', 'initial_epsilon': 5.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Madame Frostina', 'may_query': True, 'datasets_list': [{'dataset_name': 'IRIS', 'initial_epsilon': 5.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}, {'dataset_name': 'PENGUIN', 'initial_epsilon': 5.0, 'initial_delta': 0.005, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}, {'user_name': 'Dr. Antartica', 'may_query': True, 'datasets_list': [{'dataset_name': 'PENGUIN', 'initial_epsilon': 20.0, 'initial_delta': 0.001, 'total_spent_epsilon': 0.0, 'total_spent_delta': 0.0}]}]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7cfa86",
   "metadata": {},
   "source": [
    "### Finally, everything can actually be loaded directly from a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43340fc9",
   "metadata": {},
   "source": [
    "Let's delete the existing user collection first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "597cb0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted collection users.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py drop_collection --collection users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81661298",
   "metadata": {},
   "source": [
    "Is is now empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1638145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b3cd2c",
   "metadata": {},
   "source": [
    "We add the data based on a yaml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87b776f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user data from yaml at ../data/collections/user_collection.yaml.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py create_users_collection --path ../data/collections/user_collection.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ccbe8",
   "metadata": {},
   "source": [
    "By default, *create_users_collection* will only add new users to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a84c9392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new users added, they already exist in the server\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py create_users_collection --path ../data/collections/user_collection.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ca747",
   "metadata": {},
   "source": [
    "If you want to clean the current users collection and replace it, you can use the argument *--clean*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bb9a70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning done. \n",
      "\n",
      "Added user data from yaml at ../data/collections/user_collection.yaml.\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py create_users_collection --path ../data/collections/user_collection.yaml --clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371742a4",
   "metadata": {},
   "source": [
    "If you want to add new users and update the existing ones in your collection, you can use the argument *--overwrite*. This will make sure to add new users if they do not exist and replace values from existing users with the collection provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae4ce110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing users updated. \n",
      "No new users added, they already exist in the server\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py create_users_collection --path ../data/collections/user_collection.yaml --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63853e73",
   "metadata": {},
   "source": [
    "And let's see the resulting collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77866f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_name': 'Alice', 'may_query': True, 'datasets_list': [{'dataset_name': 'IRIS', 'initial_epsilon': 10, 'initial_delta': 0.0001, 'total_spent_epsilon': 1, 'total_spent_delta': 1e-06}, {'dataset_name': 'PENGUIN', 'initial_epsilon': 5, 'initial_delta': 0.0005, 'total_spent_epsilon': 0.2, 'total_spent_delta': 1e-07}]}, {'user_name': 'Dr. Antartica', 'may_query': True, 'datasets_list': [{'dataset_name': 'PENGUIN', 'initial_epsilon': 45, 'initial_delta': 0.005, 'total_spent_epsilon': 0, 'total_spent_delta': 0}]}, {'user_name': 'Dr. FSO', 'may_query': True, 'datasets_list': [{'dataset_name': 'FSO_INCOME_SYNTHETIC', 'initial_epsilon': 45, 'initial_delta': 0.005, 'total_spent_epsilon': 0, 'total_spent_delta': 0}]}, {'user_name': 'Bob', 'may_query': True, 'datasets_list': [{'dataset_name': 'IRIS', 'initial_epsilon': 10, 'initial_delta': 0.0001, 'total_spent_epsilon': 0, 'total_spent_delta': 0}]}, {'user_name': 'Jack', 'may_query': True, 'datasets_list': [{'dataset_name': 'TITANIC', 'initial_epsilon': 45, 'initial_delta': 0.2, 'total_spent_epsilon': 0, 'total_spent_delta': 0}]}]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b58d3",
   "metadata": {},
   "source": [
    "## Archives of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ed2df36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "!python mongodb_admin.py show_collection --collection queries_archives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27be3d3-77a2-43d3-9a7f-87c8466293fe",
   "metadata": {},
   "source": [
    "## Stopping the service: Let's not do it right now!\n",
    "\n",
    "To tear down the service, we simply execute the command `helm uninstall lomas-service`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
